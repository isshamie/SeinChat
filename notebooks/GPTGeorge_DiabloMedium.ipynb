{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the DiabloGPT model based on characters from Seinfeld.\n",
    "Inspired from https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onU41i8g1J3M",
    "outputId": "b61c96e6-0802-45c5-af2c-749416e98650",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install transformers, preferably in a conda environment\n",
    "#! pip  install transformers\n",
    "#! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/isshamie/SeinChat\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = \"GEORGE\" # \"JERRY\", \"KRAMER\", \"ELAINE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DiabloGPT-medium model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "4eeff4d837764430952ee8539bf23fcc",
      "41cc58c548074c82994ef12fb6d626cd",
      "266001e6143447289fa26d9fd56d7c5d",
      "3e5ac2e481ba4431a6c277e19b5e9b94",
      "51f78ff8982c4249b9dd249b31294b12",
      "117eea57b50f4a659d4795b15b622045",
      "280b6fa9cd764129bfec640b2c7d5f5d",
      "e6fc1dd94995471095638d7f7db8bee0",
      "a450014588ff4dcab86ed54464f56feb",
      "22ef9f247d464a36bbdefcef1eea8e02",
      "5f83c677a6bc44629a6c34409ae62aa1",
      "81678748779f45b69c08198bf3eee072",
      "22667c0aa48a482393260990ab8a9a62",
      "1d9fd2176ac94ad6a3df1e49907a057c",
      "ca7eb3db3ada413da0ceb26ebd8fd68f",
      "5f49f7f3a4bc4b7fbb534306ecbdeec3",
      "66ec979f2b64410e94758f9865724d93",
      "060c534087d44a45b291ac79ea0ae647",
      "ed3a6cbfa3d04fa09ae3ca20e361d416",
      "6c7e936a35204317bbe1c2788a0712ca",
      "f39bbdfadf47402cb4d307a0593adbe7",
      "f4c5fdd4728749a19415d4ceb03d1d59",
      "6b292a776d5047dd81f387ebca08bd0c",
      "b8dc19e786154f53bc828d3d46b4b748",
      "4db83f0f72e045a5b8128ea46db8802b",
      "b13fbf92d6d14c31b08d4a83ca6eaa2c",
      "d05b19e7b81240929dd25542da49cf6c",
      "9fc28ed59ced42bc94f944be8f93e548",
      "db5491a36ec5444e89fb623cf431dc87",
      "7a9a0886c0a54514984b3e2a7d22798c",
      "4cf685b508c44fba90a018304339ba4a",
      "c69e411409b24177960ccae863b32937",
      "074098ce7e6e48e7ad92312c07ce4ebe",
      "f175189536e44699b5e3d6fd752f9245",
      "9818add4cff74a8183f1b69793bd1300",
      "ed3297d92c704305831a04d77261db56",
      "a2a6fdb3658c41c6a73a81d619b74891",
      "3282d7cf4ff446239689e13eecb58247",
      "ee47889d3884408c904144ac1ac325f4",
      "da585cd993f74611b8e6bd68a1d77dbc",
      "474f055500c64a9894a39089d0c31e54",
      "ac5fb53ef8234b73a269ff190aa46e01",
      "01dc0df2db704106bc1f5eed761f0fb7",
      "1e1aeb472789402b8898235634d454d5",
      "fbb988213c1849359ad9306df0d8236a",
      "93432a09227c4b808a292fc33c0ab10f",
      "36fc0b6107ed481696f8c47920541f13",
      "f6ebd960a33047ae896893f3b37a338b",
      "742805b3bcfb4f31981c64685b5c141e",
      "b793dfc170d64f0e85919fe3e32ed8e8",
      "94db01728b43498d9a874bc8ba8129cd",
      "8553c23ab8b24e69b83d5051aeb9218d",
      "e4e05fd989b8416e86da2d31a47bb752",
      "1c65cf5752df42c488a57ebada3a3886",
      "d37b6d42753e4068843541f3e2246462"
     ]
    },
    "id": "w6qrl7_SvPKg",
    "outputId": "4a2dc314-9da6-4fd4-dea0-ff816eccb9a1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the following to see what the dialogue looks like before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CjZaN5ilgd-z"
   },
   "outputs": [],
   "source": [
    "# # Let's chat for 5 lines\n",
    "# for step in range(5):\n",
    "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "#     # append the new user input tokens to the chat history\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     # generated a response while limiting the total chat history to 1000 tokens    \n",
    "#     chat_history_ids = model.generate(\n",
    "#     bot_input_ids, max_length=1000,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "#     # pretty print last ouput tokens from bot\n",
    "#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuzSROqxjUKM"
   },
   "source": [
    "## Model initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC3qNlfp30aU"
   },
   "source": [
    "Let's train our Seinfeld character chatbot. For start, we will need basic configuration and a dataset.\n",
    "Configuration and training scripts are mostly based on this [script](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) from Huggingface and great [tutorial](https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html) from Nathan Cooper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g91QzdqU2haO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/isshamie/miniconda3/envs/sein/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "utprDGf06OVt"
   },
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output_diablomedium'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = \"microsoft/DialoGPT-medium\"\n",
    "        self.config_name = \"microsoft/DialoGPT-medium\"\n",
    "        self.tokenizer_name = \"microsoft/DialoGPT-medium\"\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 8\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 15000\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_qYqlTe9yx2"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "TxHdyv67dptm",
    "outputId": "810482a3-1add-4372-d80e-efaa664e9110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>EpisodeNo</th>\n",
       "      <th>SEID</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Do you know what this is all about? Do you kno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>(pointing at Georges shirt) See, to me, that b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are you through?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Oh, you dont recall?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>(on an imaginary microphone) Uh, no, not at th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Well, senator, Id just like to know, what you ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CLAIRE</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character                                           Dialogue  EpisodeNo  \\\n",
       "0     JERRY  Do you know what this is all about? Do you kno...        1.0   \n",
       "1     JERRY  (pointing at Georges shirt) See, to me, that b...        1.0   \n",
       "2    GEORGE                                   Are you through?        1.0   \n",
       "3     JERRY             You do of course try on, when you buy?        1.0   \n",
       "4    GEORGE  Yes, it was purple, I liked it, I dont actuall...        1.0   \n",
       "5     JERRY                               Oh, you dont recall?        1.0   \n",
       "6    GEORGE  (on an imaginary microphone) Uh, no, not at th...        1.0   \n",
       "7     JERRY  Well, senator, Id just like to know, what you ...        1.0   \n",
       "8    CLAIRE                        Mr. Seinfeld. Mr. Costanza.        1.0   \n",
       "9    GEORGE  Are, are you sure this is decaf? Wheres the or...        1.0   \n",
       "\n",
       "     SEID  Season  \n",
       "0  S01E01     1.0  \n",
       "1  S01E01     1.0  \n",
       "2  S01E01     1.0  \n",
       "3  S01E01     1.0  \n",
       "4  S01E01     1.0  \n",
       "5  S01E01     1.0  \n",
       "6  S01E01     1.0  \n",
       "7  S01E01     1.0  \n",
       "8  S01E01     1.0  \n",
       "9  S01E01     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at original dataset\n",
    "all_sein = pd.read_csv('data/sein_scripts_kaggle/scripts.csv', index_col=0)\n",
    "all_sein.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTa8wivhxK8e"
   },
   "source": [
    "### Remove the parenthesis in Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KY0wr2csxquf",
    "outputId": "24555276-1805-43d5-9df6-8e2ed26e5dd9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# text = \"heyu (rherhe ) (rjerj)\"\n",
    "# print(str(re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", text)))\n",
    "\n",
    "def rm_parenthesis(text):\n",
    "    return str(re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", text)).replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "all_sein['Dialogue'] = all_sein['Dialogue'].fillna(\"\").apply(rm_parenthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5APmLbe8yeO9",
    "outputId": "823f8ddf-0d0b-4343-e7b0-eb478b519292"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>EpisodeNo</th>\n",
       "      <th>SEID</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Do you know what this is all about? Do you kno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>See, to me, that button is in the worst possi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are you through?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character                                           Dialogue  EpisodeNo  \\\n",
       "0     JERRY  Do you know what this is all about? Do you kno...        1.0   \n",
       "1     JERRY   See, to me, that button is in the worst possi...        1.0   \n",
       "2    GEORGE                                   Are you through?        1.0   \n",
       "3     JERRY             You do of course try on, when you buy?        1.0   \n",
       "4    GEORGE  Yes, it was purple, I liked it, I dont actuall...        1.0   \n",
       "\n",
       "     SEID  Season  \n",
       "0  S01E01     1.0  \n",
       "1  S01E01     1.0  \n",
       "2  S01E01     1.0  \n",
       "3  S01E01     1.0  \n",
       "4  S01E01     1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sein.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rBKxZCCeQelq"
   },
   "outputs": [],
   "source": [
    "all_sein[\"Character\"] = all_sein[\"Character\"].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n51hsl2mAG8v"
   },
   "source": [
    "We will convert this dataset in a way that every responce row will contain **n** previous responces as a context. For our purposes seven previous responces will be enough.\n",
    "This runs for each character, so it gets a character response and the seven previous lines of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyL7Dx1bbLRC",
    "outputId": "8d49b17b-1e15-461f-fc3e-35b47b6c4366"
   },
   "outputs": [],
   "source": [
    "def run_context(character,  n=7, verbose=False):\n",
    "    contexted = []\n",
    "    # find the indices where the character speaks, break down for each episode so no cross over\n",
    "    episodes_groups = all_sein.groupby(\"SEID\")\n",
    "    for ep, ep_df in episodes_groups:\n",
    "        ep_df = ep_df.reset_index()\n",
    "        curr_char_inds = ep_df[ep_df[\"Character\"] == character].index\n",
    "        if verbose:\n",
    "            print(f\"number of {character} lines in episode {ep}: {len(curr_char_inds)}\")\n",
    "        for i_line in curr_char_inds:\n",
    "            if i_line <= n: # too early in episode. \n",
    "                continue\n",
    "            row = []\n",
    "            prev = i_line - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "            for j in range(i_line, prev, -1):\n",
    "                row.append(ep_df['Dialogue'][j])\n",
    "            contexted.append(row)        \n",
    "    return contexted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print('running jerry')\n",
    "# jerry_contexted = run_context(\"JERRY\", n=n, verbose=False)\n",
    "# print('running george')\n",
    "# george_contexted = run_context(\"GEORGE\", n=n, verbose=False)\n",
    "\n",
    "# jerry_contexted[:5]\n",
    "\n",
    "# # Rick n mortys old was 1898\n",
    "# contexted = george_contexted\n",
    "# len(contexted)\n",
    "\n",
    "# print(len(george_contexted))\n",
    "# print(len(jerry_contexted))\n",
    "\n",
    "\n",
    "contexted = run_context(character, n=n, verbose=False)\n",
    "print(len(character))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNkYLty-VhTO",
    "outputId": "06b111b4-b7d3-4c76-a7be-84b7a34704a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response',\n",
       " 'context',\n",
       " 'context/0',\n",
       " 'context/1',\n",
       " 'context/2',\n",
       " 'context/3',\n",
       " 'context/4',\n",
       " 'context/5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "kPafxqIYgurW",
    "outputId": "900cafe5-3c37-4c60-8160-7838d2c34875",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "      <td>Well, senator, Id just like to know, what you ...</td>\n",
       "      <td>Uh, no, not at this time.</td>\n",
       "      <td>Oh, you dont recall?</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>Are you through?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "      <td>Can you relax, its a cup of coffee. Claire is ...</td>\n",
       "      <td>Its missing, I have to do it in my head decaf ...</td>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "      <td>Well, senator, Id just like to know, what you ...</td>\n",
       "      <td>Uh, no, not at this time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wait a second, wait a second, what coming in, ...</td>\n",
       "      <td>Well, theres this uh, woman might be comin in.</td>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "      <td>Can you relax, its a cup of coffee. Claire is ...</td>\n",
       "      <td>Its missing, I have to do it in my head decaf ...</td>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No, you didnt!</td>\n",
       "      <td>I told you about Laura, the girl I met in Mich...</td>\n",
       "      <td>Wait a second, wait a second, what coming in, ...</td>\n",
       "      <td>Well, theres this uh, woman might be comin in.</td>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "      <td>Can you relax, its a cup of coffee. Claire is ...</td>\n",
       "      <td>Its missing, I have to do it in my head decaf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ha.</td>\n",
       "      <td>I thought I told you about it, yes, she teache...</td>\n",
       "      <td>No, you didnt!</td>\n",
       "      <td>I told you about Laura, the girl I met in Mich...</td>\n",
       "      <td>Wait a second, wait a second, what coming in, ...</td>\n",
       "      <td>Well, theres this uh, woman might be comin in.</td>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Are, are you sure this is decaf? Wheres the or...   \n",
       "1  How come youre not doin the second show tomorrow?   \n",
       "2  Wait a second, wait a second, what coming in, ...   \n",
       "3                                     No, you didnt!   \n",
       "4                                                Ha.   \n",
       "\n",
       "                                             context  \\\n",
       "0                        Mr. Seinfeld. Mr. Costanza.   \n",
       "1  Trust me George. No one has any interest in se...   \n",
       "2     Well, theres this uh, woman might be comin in.   \n",
       "3  I told you about Laura, the girl I met in Mich...   \n",
       "4  I thought I told you about it, yes, she teache...   \n",
       "\n",
       "                                           context/0  \\\n",
       "0  Well, senator, Id just like to know, what you ...   \n",
       "1  Can you relax, its a cup of coffee. Claire is ...   \n",
       "2  How come youre not doin the second show tomorrow?   \n",
       "3  Wait a second, wait a second, what coming in, ...   \n",
       "4                                     No, you didnt!   \n",
       "\n",
       "                                           context/1  \\\n",
       "0                          Uh, no, not at this time.   \n",
       "1  Its missing, I have to do it in my head decaf ...   \n",
       "2  Trust me George. No one has any interest in se...   \n",
       "3     Well, theres this uh, woman might be comin in.   \n",
       "4  I told you about Laura, the girl I met in Mich...   \n",
       "\n",
       "                                           context/2  \\\n",
       "0                               Oh, you dont recall?   \n",
       "1  Are, are you sure this is decaf? Wheres the or...   \n",
       "2  Can you relax, its a cup of coffee. Claire is ...   \n",
       "3  How come youre not doin the second show tomorrow?   \n",
       "4  Wait a second, wait a second, what coming in, ...   \n",
       "\n",
       "                                           context/3  \\\n",
       "0  Yes, it was purple, I liked it, I dont actuall...   \n",
       "1                        Mr. Seinfeld. Mr. Costanza.   \n",
       "2  Its missing, I have to do it in my head decaf ...   \n",
       "3  Trust me George. No one has any interest in se...   \n",
       "4     Well, theres this uh, woman might be comin in.   \n",
       "\n",
       "                                           context/4  \\\n",
       "0             You do of course try on, when you buy?   \n",
       "1  Well, senator, Id just like to know, what you ...   \n",
       "2  Are, are you sure this is decaf? Wheres the or...   \n",
       "3  Can you relax, its a cup of coffee. Claire is ...   \n",
       "4  How come youre not doin the second show tomorrow?   \n",
       "\n",
       "                                           context/5  \n",
       "0                                   Are you through?  \n",
       "1                          Uh, no, not at this time.  \n",
       "2                        Mr. Seinfeld. Mr. Costanza.  \n",
       "3  Its missing, I have to do it in my head decaf ...  \n",
       "4  Trust me George. No one has any interest in se...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBeM8pvEjigq"
   },
   "source": [
    "Split our dataset into a training and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "g1CeutVVlL85",
    "outputId": "4653e6c3-e65d-4e97-a164-32d2b60d7f00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>I can't tell anymore.</td>\n",
       "      <td>You don't know?</td>\n",
       "      <td>I think so.</td>\n",
       "      <td>So you like her?</td>\n",
       "      <td>Great.</td>\n",
       "      <td>Is she nice?</td>\n",
       "      <td>Karen.</td>\n",
       "      <td>So, what's her name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td>Everything?</td>\n",
       "      <td>Nonsense. You do everything wrong.</td>\n",
       "      <td>Feel like I can't do anything wrong.</td>\n",
       "      <td>Aw, come on there now.</td>\n",
       "      <td>Never thought I'd fail at failing.</td>\n",
       "      <td>How could they not fire you?</td>\n",
       "      <td>That he may be. But he's outta my life, starti...</td>\n",
       "      <td>I gotta invite Jerry. He's my buddy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>Hu humm? i suppose we could go to Lincoln Cent...</td>\n",
       "      <td>Not a wit.</td>\n",
       "      <td>I suppose I could just pull this out and walk ...</td>\n",
       "      <td>That's right I don't.</td>\n",
       "      <td>Of course not. You don't care what I look like.</td>\n",
       "      <td>It doesn't..</td>\n",
       "      <td>No...Why should that make any difference to you?</td>\n",
       "      <td>Hey! Look , no shave.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>Leslie.</td>\n",
       "      <td>Kramer, Kramer, Kramer..</td>\n",
       "      <td>About what?</td>\n",
       "      <td>George, don't even think about it! Don't even...</td>\n",
       "      <td>Well, Leslie, sometimes the road less travelle...</td>\n",
       "      <td>Jerry, what a surprise! I thought you sere out...</td>\n",
       "      <td>I can't believe you told Kramer it's okay to p...</td>\n",
       "      <td>The show was cancelled. There was a blizzard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8844</th>\n",
       "      <td>God, you're like a rock star.</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>And you threw it out the window?</td>\n",
       "      <td>So Joe Mayo had the same coat.</td>\n",
       "      <td>Oh. Uhhh...</td>\n",
       "      <td>Hey, I got a coat just like this!</td>\n",
       "      <td>What?</td>\n",
       "      <td>All right, let's hit the bricks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               response  \\\n",
       "3687                              I can't tell anymore.   \n",
       "8079                                        Everything?   \n",
       "5760  Hu humm? i suppose we could go to Lincoln Cent...   \n",
       "772                                             Leslie.   \n",
       "8844                      God, you're like a rock star.   \n",
       "\n",
       "                                 context  \\\n",
       "3687                     You don't know?   \n",
       "8079  Nonsense. You do everything wrong.   \n",
       "5760                          Not a wit.   \n",
       "772             Kramer, Kramer, Kramer..   \n",
       "8844                             Mm-hmm.   \n",
       "\n",
       "                                              context/0  \\\n",
       "3687                                        I think so.   \n",
       "8079               Feel like I can't do anything wrong.   \n",
       "5760  I suppose I could just pull this out and walk ...   \n",
       "772                                         About what?   \n",
       "8844                   And you threw it out the window?   \n",
       "\n",
       "                                              context/1  \\\n",
       "3687                                   So you like her?   \n",
       "8079                             Aw, come on there now.   \n",
       "5760                              That's right I don't.   \n",
       "772    George, don't even think about it! Don't even...   \n",
       "8844                     So Joe Mayo had the same coat.   \n",
       "\n",
       "                                              context/2  \\\n",
       "3687                                             Great.   \n",
       "8079                 Never thought I'd fail at failing.   \n",
       "5760    Of course not. You don't care what I look like.   \n",
       "772   Well, Leslie, sometimes the road less travelle...   \n",
       "8844                                        Oh. Uhhh...   \n",
       "\n",
       "                                              context/3  \\\n",
       "3687                                       Is she nice?   \n",
       "8079                       How could they not fire you?   \n",
       "5760                                       It doesn't..   \n",
       "772   Jerry, what a surprise! I thought you sere out...   \n",
       "8844                  Hey, I got a coat just like this!   \n",
       "\n",
       "                                              context/4  \\\n",
       "3687                                             Karen.   \n",
       "8079  That he may be. But he's outta my life, starti...   \n",
       "5760   No...Why should that make any difference to you?   \n",
       "772   I can't believe you told Kramer it's okay to p...   \n",
       "8844                                              What?   \n",
       "\n",
       "                                          context/5  \n",
       "3687                           So, what's her name?  \n",
       "8079           I gotta invite Jerry. He's my buddy.  \n",
       "5760                          Hey! Look , no shave.  \n",
       "772   The show was cancelled. There was a blizzard.  \n",
       "8844               All right, let's hit the bricks.  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size = 0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86F3WhnFO4H8"
   },
   "source": [
    "Now will convert our dataset in a format suitable for our model. Basically we will concatenate responses in one string for each row (additionally we will add special 'end of string' token between responses, so the model will understand end of each response in a string).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PX7jeWpYmOe_"
   },
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        #block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "naaRHoXgnStq"
   },
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkvMNnrnVHQw"
   },
   "source": [
    "## Training and Evaluating\n",
    "\n",
    "There will be quite a lot of code needed for training our model but don’t worry, everything should work as is, the main thing is to give the model the dataset in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tXzKlXHeu0Mb"
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "                    #tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "        \n",
    "    plt.plot(tr_loss/global_step)\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-MGD6bFXV4Z-"
   },
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "d42bf870d0f840228ac0304e6e53842d",
      "39ea1a0b93994306af7cd82967f02830",
      "cd897d6352fc4acab0b1f3c0d0ff314d",
      "8bd711e6478845c5923a563688c8467b",
      "356641d35ef8400685ed0a4235564f4a",
      "410b38acea974741b2cd2409917a3c70",
      "8dfaed278e8a46c79aa414094d16a449",
      "014b854604af492ab920b039c5ebf290",
      "a5d2f4d7e28e4db7b2b4a32da6fd549d",
      "e1d84e2aaa9047c1b1cd82fb41dfbfef",
      "a2be4760042d4c2385254e770423633e",
      "5e3d4d2b02c24821ae764051b0f50e04",
      "c07ab76afdbf474199349209b1b81ca4",
      "dba6f6af1f9c410a9e141f17d81566fd",
      "14b79858219d49fca5a7efcdb79b7ea1",
      "6d80dc179cca4146a0ffdde18c6b3f63",
      "b21415f844f444d0aab1f3993b739aef",
      "f8f194643550458f9adc524e580677c0",
      "e55b2064cd95444eb0882d3875e2fe63",
      "b168137f9fec425fa4df0b0c82bc89ef",
      "5ee6e293f9b94bd3bdc31c0eff51caa9",
      "eb41e48ecfcf4235ab7b917686f789dd",
      "cb5d37dea1be42ca8f2f422806018c30",
      "1c529574e4cc40be907d5342071a50b1",
      "a0d4c1e47cb3488994f1880e63fa90f0",
      "1364282547414042ac39f127e2078c39",
      "3cf6bf9c0319439d889a1cf5dd568884",
      "4a4b2cbd11944a4abea07f11d83e07d5",
      "50c7dd4f66a8470e8fc511c1f23ad699",
      "a2aa60e66d1047139318969f4795766f",
      "73b0954583ee4d68a50a110b0e7e1575",
      "a6d3cbc6c3204983bbdd50e9ced7bdbe",
      "b6970e15d00145cca8ff0ccf4bc627a5",
      "ce2d41c87ed5407c83447e0056566b3c",
      "ea0457579fac4b3db537dcb435112363",
      "4f18902f775d453d8642c6dea23372ff",
      "ffed65739f5f441eab26d1c152ddc915",
      "420d46774cdc4e1f9a5b080a43589ed3",
      "a1431a10e0d0453492340e2566b9bfa4",
      "9141707af7524a1ba13cc70c6ebb9694",
      "a167439f9ab647549049b34a28974e47",
      "a3c9a93327494665ac917164caa0bfcf",
      "393e986c61bf42af88314e18c763bb6a",
      "a5d35351f237441880037b2e17cf736c",
      "df0803248cd24f7d8e873463f49fa50a"
     ]
    },
    "id": "__iqR8YFV-Ex",
    "outputId": "38f49440-70b5-4db4-cbcd-1c806190a62c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/24/2022 19:31:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "08/24/2022 19:31:47 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f17acc9f2e0>\n",
      "08/24/2022 19:31:47 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "08/24/2022 19:31:51 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "08/24/2022 19:31:51 - INFO - __main__ -   ***** Running training *****\n",
      "08/24/2022 19:31:51 - INFO - __main__ -     Num examples = 8443\n",
      "08/24/2022 19:31:51 - INFO - __main__ -     Num Epochs = 8\n",
      "08/24/2022 19:31:51 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "08/24/2022 19:31:51 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "08/24/2022 19:31:51 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "08/24/2022 19:31:51 - INFO - __main__ -     Total optimization steps = 16880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbab24a718c45f794c91b6f630cbfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db49b03e21114e77849dc69564ff1a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a64b4184c6a47d89ffca7cffacb7763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb71b08aee85409195cd54f331e5496f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1d4eb32e694fd2a87cd664e8b8a243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1349258db9ea42019f9cecb3d09b57ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489f4cbde33d4fd988a293613db27092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116d93ce23e54315a5691ffae62565df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6675e90b9a418f979aa81bb133d83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/24/2022 20:19:27 - INFO - __main__ -   Saving model checkpoint to output_diablomedium/checkpoint-15000\n",
      "08/24/2022 20:19:50 - INFO - __main__ -   Saving optimizer and scheduler states to output_diablomedium/checkpoint-15000\n",
      "08/24/2022 20:25:46 - INFO - __main__ -    global_step = 16880, average loss = 1.0974718067547862\n",
      "08/24/2022 20:25:46 - INFO - __main__ -   Saving model checkpoint to output_diablomedium\n",
      "08/24/2022 20:26:00 - INFO - __main__ -   Evaluate the following checkpoints: ['output_diablomedium']\n",
      "08/24/2022 20:26:03 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "08/24/2022 20:26:03 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "08/24/2022 20:26:03 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "08/24/2022 20:26:03 - INFO - __main__ -     Num examples = 939\n",
      "08/24/2022 20:26:03 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbf27e13a1840798fb631943f9c99f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/24/2022 20:26:13 - INFO - __main__ -   ***** Eval results  *****\n",
      "08/24/2022 20:26:13 - INFO - __main__ -     perplexity = tensor(2.6779)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(2.6779)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eDkPEuvbD47"
   },
   "source": [
    "# Chatting with the character!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjVqotI05gOS"
   },
   "source": [
    "The model is ready, so it's time to chat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Who are you?\n",
      "SeinBot: I'm him.\n",
      ">> User:What do you do for a living?\n",
      "SeinBot: Im just a guy who loves watches.\n",
      ">> User:What kind of watch is your favorite?\n",
      "SeinBot: An alarm clock.\n",
      ">> User:That's cool. Do you think Stacy will like the alarm clock you got her?\n",
      "SeinBot: Oh yeah.\n",
      ">> User:But she asked you for Broadway tickets. What are you going to tell her?\n",
      "SeinBot: !!!,!!?!!.!!...!!..!!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=1000, \n",
    "        top_p=0.7,\n",
    "        num_beams=10,\n",
    "        temperature = 0.9,\n",
    "        #num_beams=5, \n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:What is your name?\n",
      "SeinBot: ..Alaines Costanza.\n",
      ">> User:What do you do for a living?\n",
      "SeinBot: I'm an architect.\n",
      ">> User:What buildings have you designed?\n",
      "SeinBot: Uh, the library, uh.\n",
      ">> User:Oh wow, that's so cool. Which library was it?\n",
      "SeinBot: !!! remindme 3 days\n",
      ">> User:Tell me a joke\n",
      "SeinBot: ! remind me 3 days!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=10000, \n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Who are you?\n",
      "SeinBot: I'm the guy who lives here.\n",
      ">> User:What do you do for a living? \n",
      "SeinBot: I sell bras.\n",
      ">> User:Oh wow, that's an interesting job. Do you meet a lot of women at that job?\n",
      "SeinBot: Yeah, I've been going there for 10 years.\n",
      ">> User:Are you seeing anyone now?\n",
      "SeinBot: !!!?!!,!!\n",
      ">> User:Tell me a joke\n",
      "SeinBot: !?,!,,!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=500, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.9,\n",
    "        #num_beams=5, \n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:How is your day going George? \n",
      "SeinBot: Actually, it's going pretty good.\n",
      ">> User:Oh nice. Did you talk to Steinbrenner in the end?\n",
      "SeinBot: Yeah, it was very nice.\n",
      ">> User:Which trade proposal did he like?\n",
      "SeinBot: The one about the car.\n",
      ">> User:No did he like the Jeter or the O'neill trade?\n",
      "SeinBot: I don't know.\n",
      ">> User:So what did you talk about in the meeting?\n",
      "SeinBot: !!!?!!,!!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=10000, \n",
    "        num_beams=15,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNPpVtamLDKn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "1jAMH0XPuE-f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjd1pCZxukWh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofg7iJNFyjBs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPTSein.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
