{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onU41i8g1J3M",
    "outputId": "b61c96e6-0802-45c5-af2c-749416e98650",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install transformers, preferably in a conda environment\n",
    "#! pip  install transformers\n",
    "#! pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the DiabloGPT model based on characters from Seinfeld.\n",
    "Inspired from https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/isshamie/SeinChat\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = \"GEORGE\" # \"JERRY\", \"KRAMER\", \"ELAINE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DiabloGPT-medium model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "4eeff4d837764430952ee8539bf23fcc",
      "41cc58c548074c82994ef12fb6d626cd",
      "266001e6143447289fa26d9fd56d7c5d",
      "3e5ac2e481ba4431a6c277e19b5e9b94",
      "51f78ff8982c4249b9dd249b31294b12",
      "117eea57b50f4a659d4795b15b622045",
      "280b6fa9cd764129bfec640b2c7d5f5d",
      "e6fc1dd94995471095638d7f7db8bee0",
      "a450014588ff4dcab86ed54464f56feb",
      "22ef9f247d464a36bbdefcef1eea8e02",
      "5f83c677a6bc44629a6c34409ae62aa1",
      "81678748779f45b69c08198bf3eee072",
      "22667c0aa48a482393260990ab8a9a62",
      "1d9fd2176ac94ad6a3df1e49907a057c",
      "ca7eb3db3ada413da0ceb26ebd8fd68f",
      "5f49f7f3a4bc4b7fbb534306ecbdeec3",
      "66ec979f2b64410e94758f9865724d93",
      "060c534087d44a45b291ac79ea0ae647",
      "ed3a6cbfa3d04fa09ae3ca20e361d416",
      "6c7e936a35204317bbe1c2788a0712ca",
      "f39bbdfadf47402cb4d307a0593adbe7",
      "f4c5fdd4728749a19415d4ceb03d1d59",
      "6b292a776d5047dd81f387ebca08bd0c",
      "b8dc19e786154f53bc828d3d46b4b748",
      "4db83f0f72e045a5b8128ea46db8802b",
      "b13fbf92d6d14c31b08d4a83ca6eaa2c",
      "d05b19e7b81240929dd25542da49cf6c",
      "9fc28ed59ced42bc94f944be8f93e548",
      "db5491a36ec5444e89fb623cf431dc87",
      "7a9a0886c0a54514984b3e2a7d22798c",
      "4cf685b508c44fba90a018304339ba4a",
      "c69e411409b24177960ccae863b32937",
      "074098ce7e6e48e7ad92312c07ce4ebe",
      "f175189536e44699b5e3d6fd752f9245",
      "9818add4cff74a8183f1b69793bd1300",
      "ed3297d92c704305831a04d77261db56",
      "a2a6fdb3658c41c6a73a81d619b74891",
      "3282d7cf4ff446239689e13eecb58247",
      "ee47889d3884408c904144ac1ac325f4",
      "da585cd993f74611b8e6bd68a1d77dbc",
      "474f055500c64a9894a39089d0c31e54",
      "ac5fb53ef8234b73a269ff190aa46e01",
      "01dc0df2db704106bc1f5eed761f0fb7",
      "1e1aeb472789402b8898235634d454d5",
      "fbb988213c1849359ad9306df0d8236a",
      "93432a09227c4b808a292fc33c0ab10f",
      "36fc0b6107ed481696f8c47920541f13",
      "f6ebd960a33047ae896893f3b37a338b",
      "742805b3bcfb4f31981c64685b5c141e",
      "b793dfc170d64f0e85919fe3e32ed8e8",
      "94db01728b43498d9a874bc8ba8129cd",
      "8553c23ab8b24e69b83d5051aeb9218d",
      "e4e05fd989b8416e86da2d31a47bb752",
      "1c65cf5752df42c488a57ebada3a3886",
      "d37b6d42753e4068843541f3e2246462"
     ]
    },
    "id": "w6qrl7_SvPKg",
    "outputId": "4a2dc314-9da6-4fd4-dea0-ff816eccb9a1",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398cf2c6788649f183ce50e91d29bd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5f1a3abdc7467b9809445bd3b23af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7e7179b551483f86e133e6607f5251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5b80d6febd41e4a8d890c4bb653802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e27754f00c4d259963412d77a3ebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/823M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the following to see what the dialogue looks like before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CjZaN5ilgd-z"
   },
   "outputs": [],
   "source": [
    "# # Let's chat for 5 lines\n",
    "# for step in range(5):\n",
    "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "#     # append the new user input tokens to the chat history\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     # generated a response while limiting the total chat history to 1000 tokens    \n",
    "#     chat_history_ids = model.generate(\n",
    "#     bot_input_ids, max_length=1000,\n",
    "#     pad_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "#     # pretty print last ouput tokens from bot\n",
    "#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuzSROqxjUKM"
   },
   "source": [
    "## Model initial configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC3qNlfp30aU"
   },
   "source": [
    "Let's train our Seinfeld character chatbot. For start, we will need basic configuration and a dataset.\n",
    "Configuration and training scripts are mostly based on this [script](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) from Huggingface and great [tutorial](https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html) from Nathan Cooper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g91QzdqU2haO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/isshamie/miniconda3/envs/sein/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "utprDGf06OVt"
   },
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output_diablomedium'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = \"microsoft/DialoGPT-medium\"\n",
    "        self.config_name = \"microsoft/DialoGPT-medium\"\n",
    "        self.tokenizer_name = \"microsoft/DialoGPT-medium\"\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 8\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 15000\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_qYqlTe9yx2"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "TxHdyv67dptm",
    "outputId": "810482a3-1add-4372-d80e-efaa664e9110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>EpisodeNo</th>\n",
       "      <th>SEID</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Do you know what this is all about? Do you kno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>(pointing at Georges shirt) See, to me, that b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are you through?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Oh, you dont recall?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>(on an imaginary microphone) Uh, no, not at th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Well, senator, Id just like to know, what you ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CLAIRE</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character                                           Dialogue  EpisodeNo  \\\n",
       "0     JERRY  Do you know what this is all about? Do you kno...        1.0   \n",
       "1     JERRY  (pointing at Georges shirt) See, to me, that b...        1.0   \n",
       "2    GEORGE                                   Are you through?        1.0   \n",
       "3     JERRY             You do of course try on, when you buy?        1.0   \n",
       "4    GEORGE  Yes, it was purple, I liked it, I dont actuall...        1.0   \n",
       "5     JERRY                               Oh, you dont recall?        1.0   \n",
       "6    GEORGE  (on an imaginary microphone) Uh, no, not at th...        1.0   \n",
       "7     JERRY  Well, senator, Id just like to know, what you ...        1.0   \n",
       "8    CLAIRE                        Mr. Seinfeld. Mr. Costanza.        1.0   \n",
       "9    GEORGE  Are, are you sure this is decaf? Wheres the or...        1.0   \n",
       "\n",
       "     SEID  Season  \n",
       "0  S01E01     1.0  \n",
       "1  S01E01     1.0  \n",
       "2  S01E01     1.0  \n",
       "3  S01E01     1.0  \n",
       "4  S01E01     1.0  \n",
       "5  S01E01     1.0  \n",
       "6  S01E01     1.0  \n",
       "7  S01E01     1.0  \n",
       "8  S01E01     1.0  \n",
       "9  S01E01     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at original dataset\n",
    "all_sein = pd.read_csv('data/sein_scripts_kaggle/scripts.csv', index_col=0)\n",
    "all_sein.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTa8wivhxK8e"
   },
   "source": [
    "### Remove the parenthesis in Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KY0wr2csxquf",
    "outputId": "24555276-1805-43d5-9df6-8e2ed26e5dd9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# text = \"heyu (rherhe ) (rjerj)\"\n",
    "# print(str(re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", text)))\n",
    "\n",
    "def rm_parenthesis(text):\n",
    "    return str(re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", text)).replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "all_sein['Dialogue'] = all_sein['Dialogue'].fillna(\"\").apply(rm_parenthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5APmLbe8yeO9",
    "outputId": "823f8ddf-0d0b-4343-e7b0-eb478b519292"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Dialogue</th>\n",
       "      <th>EpisodeNo</th>\n",
       "      <th>SEID</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>Do you know what this is all about? Do you kno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>See, to me, that button is in the worst possi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Are you through?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JERRY</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEORGE</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S01E01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Character                                           Dialogue  EpisodeNo  \\\n",
       "0     JERRY  Do you know what this is all about? Do you kno...        1.0   \n",
       "1     JERRY   See, to me, that button is in the worst possi...        1.0   \n",
       "2    GEORGE                                   Are you through?        1.0   \n",
       "3     JERRY             You do of course try on, when you buy?        1.0   \n",
       "4    GEORGE  Yes, it was purple, I liked it, I dont actuall...        1.0   \n",
       "\n",
       "     SEID  Season  \n",
       "0  S01E01     1.0  \n",
       "1  S01E01     1.0  \n",
       "2  S01E01     1.0  \n",
       "3  S01E01     1.0  \n",
       "4  S01E01     1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sein.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rBKxZCCeQelq"
   },
   "outputs": [],
   "source": [
    "all_sein[\"Character\"] = all_sein[\"Character\"].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n51hsl2mAG8v"
   },
   "source": [
    "We will convert this dataset in a way that every responce row will contain **n** previous responces as a context. For our purposes seven previous responces will be enough.\n",
    "This runs for each character, so it gets a character response and the seven previous lines of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyL7Dx1bbLRC",
    "outputId": "8d49b17b-1e15-461f-fc3e-35b47b6c4366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running jerry\n",
      "running george\n"
     ]
    }
   ],
   "source": [
    "def run_context(character,  n=7, verbose=False):\n",
    "    contexted = []\n",
    "    # find the indices where the character speaks, break down for each episode so no cross over\n",
    "    episodes_groups = all_sein.groupby(\"SEID\")\n",
    "    for ep, ep_df in episodes_groups:\n",
    "        ep_df = ep_df.reset_index()\n",
    "        curr_char_inds = ep_df[ep_df[\"Character\"] == character].index\n",
    "        if verbose:\n",
    "            print(f\"number of {character} lines in episode {ep}: {len(curr_char_inds)}\")\n",
    "        for i_line in curr_char_inds:\n",
    "            if i_line <= n: # too early in episode. \n",
    "                continue\n",
    "            row = []\n",
    "            prev = i_line - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "            for j in range(i_line, prev, -1):\n",
    "                row.append(ep_df['Dialogue'][j])\n",
    "            contexted.append(row)        \n",
    "    return contexted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('running jerry')\n",
    "# jerry_contexted = run_context(\"JERRY\", n=n, verbose=False)\n",
    "# print('running george')\n",
    "# george_contexted = run_context(\"GEORGE\", n=n, verbose=False)\n",
    "\n",
    "# jerry_contexted[:5]\n",
    "\n",
    "# # Rick n mortys old was 1898\n",
    "# contexted = george_contexted\n",
    "# len(contexted)\n",
    "\n",
    "# print(len(george_contexted))\n",
    "# print(len(jerry_contexted))\n",
    "\n",
    "\n",
    "contexted = run_context(character, n=n, verbose=False)\n",
    "print(len(character))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNkYLty-VhTO",
    "outputId": "06b111b4-b7d3-4c76-a7be-84b7a34704a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response',\n",
       " 'context',\n",
       " 'context/0',\n",
       " 'context/1',\n",
       " 'context/2',\n",
       " 'context/3',\n",
       " 'context/4',\n",
       " 'context/5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/'+str(i) for i in range(n-1)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "kPafxqIYgurW",
    "outputId": "900cafe5-3c37-4c60-8160-7838d2c34875",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "      <td>Well, senator, Id just like to know, what you ...</td>\n",
       "      <td>Uh, no, not at this time.</td>\n",
       "      <td>Oh, you dont recall?</td>\n",
       "      <td>Yes, it was purple, I liked it, I dont actuall...</td>\n",
       "      <td>You do of course try on, when you buy?</td>\n",
       "      <td>Are you through?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "      <td>Can you relax, its a cup of coffee. Claire is ...</td>\n",
       "      <td>Its missing, I have to do it in my head decaf ...</td>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "      <td>Well, senator, Id just like to know, what you ...</td>\n",
       "      <td>Uh, no, not at this time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wait a second, wait a second, what coming in, ...</td>\n",
       "      <td>Well, theres this uh, woman might be comin in.</td>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "      <td>Can you relax, its a cup of coffee. Claire is ...</td>\n",
       "      <td>Its missing, I have to do it in my head decaf ...</td>\n",
       "      <td>Are, are you sure this is decaf? Wheres the or...</td>\n",
       "      <td>Mr. Seinfeld. Mr. Costanza.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No, you didnt!</td>\n",
       "      <td>I told you about Laura, the girl I met in Mich...</td>\n",
       "      <td>Wait a second, wait a second, what coming in, ...</td>\n",
       "      <td>Well, theres this uh, woman might be comin in.</td>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "      <td>Can you relax, its a cup of coffee. Claire is ...</td>\n",
       "      <td>Its missing, I have to do it in my head decaf ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ha.</td>\n",
       "      <td>I thought I told you about it, yes, she teache...</td>\n",
       "      <td>No, you didnt!</td>\n",
       "      <td>I told you about Laura, the girl I met in Mich...</td>\n",
       "      <td>Wait a second, wait a second, what coming in, ...</td>\n",
       "      <td>Well, theres this uh, woman might be comin in.</td>\n",
       "      <td>How come youre not doin the second show tomorrow?</td>\n",
       "      <td>Trust me George. No one has any interest in se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Are, are you sure this is decaf? Wheres the or...   \n",
       "1  How come youre not doin the second show tomorrow?   \n",
       "2  Wait a second, wait a second, what coming in, ...   \n",
       "3                                     No, you didnt!   \n",
       "4                                                Ha.   \n",
       "\n",
       "                                             context  \\\n",
       "0                        Mr. Seinfeld. Mr. Costanza.   \n",
       "1  Trust me George. No one has any interest in se...   \n",
       "2     Well, theres this uh, woman might be comin in.   \n",
       "3  I told you about Laura, the girl I met in Mich...   \n",
       "4  I thought I told you about it, yes, she teache...   \n",
       "\n",
       "                                           context/0  \\\n",
       "0  Well, senator, Id just like to know, what you ...   \n",
       "1  Can you relax, its a cup of coffee. Claire is ...   \n",
       "2  How come youre not doin the second show tomorrow?   \n",
       "3  Wait a second, wait a second, what coming in, ...   \n",
       "4                                     No, you didnt!   \n",
       "\n",
       "                                           context/1  \\\n",
       "0                          Uh, no, not at this time.   \n",
       "1  Its missing, I have to do it in my head decaf ...   \n",
       "2  Trust me George. No one has any interest in se...   \n",
       "3     Well, theres this uh, woman might be comin in.   \n",
       "4  I told you about Laura, the girl I met in Mich...   \n",
       "\n",
       "                                           context/2  \\\n",
       "0                               Oh, you dont recall?   \n",
       "1  Are, are you sure this is decaf? Wheres the or...   \n",
       "2  Can you relax, its a cup of coffee. Claire is ...   \n",
       "3  How come youre not doin the second show tomorrow?   \n",
       "4  Wait a second, wait a second, what coming in, ...   \n",
       "\n",
       "                                           context/3  \\\n",
       "0  Yes, it was purple, I liked it, I dont actuall...   \n",
       "1                        Mr. Seinfeld. Mr. Costanza.   \n",
       "2  Its missing, I have to do it in my head decaf ...   \n",
       "3  Trust me George. No one has any interest in se...   \n",
       "4     Well, theres this uh, woman might be comin in.   \n",
       "\n",
       "                                           context/4  \\\n",
       "0             You do of course try on, when you buy?   \n",
       "1  Well, senator, Id just like to know, what you ...   \n",
       "2  Are, are you sure this is decaf? Wheres the or...   \n",
       "3  Can you relax, its a cup of coffee. Claire is ...   \n",
       "4  How come youre not doin the second show tomorrow?   \n",
       "\n",
       "                                           context/5  \n",
       "0                                   Are you through?  \n",
       "1                          Uh, no, not at this time.  \n",
       "2                        Mr. Seinfeld. Mr. Costanza.  \n",
       "3  Its missing, I have to do it in my head decaf ...  \n",
       "4  Trust me George. No one has any interest in se...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(contexted, columns=columns)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBeM8pvEjigq"
   },
   "source": [
    "Split our dataset into a training and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "g1CeutVVlL85",
    "outputId": "4653e6c3-e65d-4e97-a164-32d2b60d7f00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>\"....I like spicy chicken\"</td>\n",
       "      <td>\"What's that?\"</td>\n",
       "      <td>\"George likes spicy chicken.\"</td>\n",
       "      <td>\"I don't know how you can eat that spicy chick...</td>\n",
       "      <td>\"'Cause when he questioned me about it I was s...</td>\n",
       "      <td>\"Why?\"</td>\n",
       "      <td>\"I have to go see Steinbrenner later. Mr Wilhe...</td>\n",
       "      <td>\"Jimmy's gonna put the moves on Elaine..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>Well now it's not gonna be original! It's gon...</td>\n",
       "      <td>I can't believe that they're using it.</td>\n",
       "      <td>What?! They're stealing the name?! That's my ...</td>\n",
       "      <td>They're gonna name their baby Seven.</td>\n",
       "      <td>So, what're you saying?</td>\n",
       "      <td>Well I was telling Carrie about our argument, ...</td>\n",
       "      <td>Why not?</td>\n",
       "      <td>Well, I dunno how original it's gonna be any m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>I couldn't take it anymore.</td>\n",
       "      <td>Get outta here.</td>\n",
       "      <td>I quit my job.</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>Kramer. Guess what.</td>\n",
       "      <td>How did you know I was here?</td>\n",
       "      <td>Guess what.</td>\n",
       "      <td>Whatever it costs. In fact, I would prefer it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>S'cuse me ..Do you mind if I ask you a few que...</td>\n",
       "      <td>Oooooh ...Pops .... Wow!! Spectacular.</td>\n",
       "      <td>Behold .. The Technicolor Dreamcoat.</td>\n",
       "      <td>Oh yeah!!</td>\n",
       "      <td>Well that's the grand tour ... Aw but I save t...</td>\n",
       "      <td>OKAY!!!</td>\n",
       "      <td>Like a shrimp farmer....</td>\n",
       "      <td>Ok....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>It wasn't that bad.</td>\n",
       "      <td>Ooo, that was a bad toast.</td>\n",
       "      <td>Do you know the last time I wore this thing? S...</td>\n",
       "      <td>Well, well, look at you. It?s a little skimpy ...</td>\n",
       "      <td>What, are you locking the door now?</td>\n",
       "      <td>It's George.</td>\n",
       "      <td>Who is it?</td>\n",
       "      <td>But officer, he threatened me! I don't unders...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               response  \\\n",
       "5712                         \"....I like spicy chicken\"   \n",
       "6654   Well now it's not gonna be original! It's gon...   \n",
       "550                         I couldn't take it anymore.   \n",
       "6947  S'cuse me ..Do you mind if I ask you a few que...   \n",
       "2861                                It wasn't that bad.   \n",
       "\n",
       "                                     context  \\\n",
       "5712                          \"What's that?\"   \n",
       "6654  I can't believe that they're using it.   \n",
       "550                          Get outta here.   \n",
       "6947  Oooooh ...Pops .... Wow!! Spectacular.   \n",
       "2861              Ooo, that was a bad toast.   \n",
       "\n",
       "                                              context/0  \\\n",
       "5712                      \"George likes spicy chicken.\"   \n",
       "6654   What?! They're stealing the name?! That's my ...   \n",
       "550                                      I quit my job.   \n",
       "6947               Behold .. The Technicolor Dreamcoat.   \n",
       "2861  Do you know the last time I wore this thing? S...   \n",
       "\n",
       "                                              context/1  \\\n",
       "5712  \"I don't know how you can eat that spicy chick...   \n",
       "6654               They're gonna name their baby Seven.   \n",
       "550                                       I don't know.   \n",
       "6947                                          Oh yeah!!   \n",
       "2861  Well, well, look at you. It?s a little skimpy ...   \n",
       "\n",
       "                                              context/2  \\\n",
       "5712  \"'Cause when he questioned me about it I was s...   \n",
       "6654                            So, what're you saying?   \n",
       "550                                 Kramer. Guess what.   \n",
       "6947  Well that's the grand tour ... Aw but I save t...   \n",
       "2861                What, are you locking the door now?   \n",
       "\n",
       "                                              context/3  \\\n",
       "5712                                             \"Why?\"   \n",
       "6654  Well I was telling Carrie about our argument, ...   \n",
       "550                        How did you know I was here?   \n",
       "6947                                            OKAY!!!   \n",
       "2861                                       It's George.   \n",
       "\n",
       "                                              context/4  \\\n",
       "5712  \"I have to go see Steinbrenner later. Mr Wilhe...   \n",
       "6654                                           Why not?   \n",
       "550                                         Guess what.   \n",
       "6947                           Like a shrimp farmer....   \n",
       "2861                                         Who is it?   \n",
       "\n",
       "                                              context/5  \n",
       "5712          \"Jimmy's gonna put the moves on Elaine..\"  \n",
       "6654  Well, I dunno how original it's gonna be any m...  \n",
       "550   Whatever it costs. In fact, I would prefer it ...  \n",
       "6947                                             Ok....  \n",
       "2861   But officer, he threatened me! I don't unders...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size = 0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86F3WhnFO4H8"
   },
   "source": [
    "Now will convert our dataset in a format suitable for our model. Basically we will concatenate responses in one string for each row (additionally we will add special 'end of string' token between responses, so the model will understand end of each response in a string).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PX7jeWpYmOe_"
   },
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        #block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "naaRHoXgnStq"
   },
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkvMNnrnVHQw"
   },
   "source": [
    "## Training and Evaluating\n",
    "\n",
    "There will be quite a lot of code needed for training our model but don’t worry, everything should work as is, the main thing is to give the model the dataset in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tXzKlXHeu0Mb"
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step)\n",
    "                    #tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "        \n",
    "    plt.plot(tr_loss/global_step)\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-MGD6bFXV4Z-"
   },
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "d42bf870d0f840228ac0304e6e53842d",
      "39ea1a0b93994306af7cd82967f02830",
      "cd897d6352fc4acab0b1f3c0d0ff314d",
      "8bd711e6478845c5923a563688c8467b",
      "356641d35ef8400685ed0a4235564f4a",
      "410b38acea974741b2cd2409917a3c70",
      "8dfaed278e8a46c79aa414094d16a449",
      "014b854604af492ab920b039c5ebf290",
      "a5d2f4d7e28e4db7b2b4a32da6fd549d",
      "e1d84e2aaa9047c1b1cd82fb41dfbfef",
      "a2be4760042d4c2385254e770423633e",
      "5e3d4d2b02c24821ae764051b0f50e04",
      "c07ab76afdbf474199349209b1b81ca4",
      "dba6f6af1f9c410a9e141f17d81566fd",
      "14b79858219d49fca5a7efcdb79b7ea1",
      "6d80dc179cca4146a0ffdde18c6b3f63",
      "b21415f844f444d0aab1f3993b739aef",
      "f8f194643550458f9adc524e580677c0",
      "e55b2064cd95444eb0882d3875e2fe63",
      "b168137f9fec425fa4df0b0c82bc89ef",
      "5ee6e293f9b94bd3bdc31c0eff51caa9",
      "eb41e48ecfcf4235ab7b917686f789dd",
      "cb5d37dea1be42ca8f2f422806018c30",
      "1c529574e4cc40be907d5342071a50b1",
      "a0d4c1e47cb3488994f1880e63fa90f0",
      "1364282547414042ac39f127e2078c39",
      "3cf6bf9c0319439d889a1cf5dd568884",
      "4a4b2cbd11944a4abea07f11d83e07d5",
      "50c7dd4f66a8470e8fc511c1f23ad699",
      "a2aa60e66d1047139318969f4795766f",
      "73b0954583ee4d68a50a110b0e7e1575",
      "a6d3cbc6c3204983bbdd50e9ced7bdbe",
      "b6970e15d00145cca8ff0ccf4bc627a5",
      "ce2d41c87ed5407c83447e0056566b3c",
      "ea0457579fac4b3db537dcb435112363",
      "4f18902f775d453d8642c6dea23372ff",
      "ffed65739f5f441eab26d1c152ddc915",
      "420d46774cdc4e1f9a5b080a43589ed3",
      "a1431a10e0d0453492340e2566b9bfa4",
      "9141707af7524a1ba13cc70c6ebb9694",
      "a167439f9ab647549049b34a28974e47",
      "a3c9a93327494665ac917164caa0bfcf",
      "393e986c61bf42af88314e18c763bb6a",
      "a5d35351f237441880037b2e17cf736c",
      "df0803248cd24f7d8e873463f49fa50a"
     ]
    },
    "id": "__iqR8YFV-Ex",
    "outputId": "38f49440-70b5-4db4-cbcd-1c806190a62c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/19/2022 12:33:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c757b6dae8486bbbf7c1f00a3cc19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c2b7597d4e4cf5a3ee4d9903c25c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25d0f26c2364399b16ad8c265d3ccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67418793523e4128a050c49cb3065b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3883ecdcce4ca2968202add0351d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/823M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/19/2022 12:33:47 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f3a4f70b3a0>\n",
      "08/19/2022 12:33:47 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "08/19/2022 12:33:50 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "08/19/2022 12:33:50 - INFO - __main__ -   ***** Running training *****\n",
      "08/19/2022 12:33:50 - INFO - __main__ -     Num examples = 8443\n",
      "08/19/2022 12:33:50 - INFO - __main__ -     Num Epochs = 8\n",
      "08/19/2022 12:33:50 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "08/19/2022 12:33:50 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "08/19/2022 12:33:50 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "08/19/2022 12:33:50 - INFO - __main__ -     Total optimization steps = 16880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718d063e8e624fa3a5b8696598ed8e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19afd960058a40839acc1b61c86a38b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eb12fb658b4d3cac72996ef40a7d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a61aafad734a059986fd818a0c3698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb734dd48f5447e81256041a666aad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa05d53c6e27490889a707cca84056e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73cdbec4eb14be59810a6905a191c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e23f635880b4c22960c5223da05dc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b7855355094777a7d6ea3c94a35473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/19/2022 13:21:19 - INFO - __main__ -   Saving model checkpoint to output_diablomedium/checkpoint-15000\n",
      "08/19/2022 13:21:22 - INFO - __main__ -   Saving optimizer and scheduler states to output_diablomedium/checkpoint-15000\n",
      "08/19/2022 13:27:18 - INFO - __main__ -    global_step = 16880, average loss = 1.1029613109323155\n",
      "08/19/2022 13:27:18 - INFO - __main__ -   Saving model checkpoint to output_diablomedium\n",
      "08/19/2022 13:27:23 - INFO - __main__ -   Evaluate the following checkpoints: ['output_diablomedium']\n",
      "08/19/2022 13:27:25 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "08/19/2022 13:27:26 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n",
      "08/19/2022 13:27:26 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "08/19/2022 13:27:26 - INFO - __main__ -     Num examples = 939\n",
      "08/19/2022 13:27:26 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68be7507a3874232826c40b8d1318325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/19/2022 13:27:36 - INFO - __main__ -   ***** Eval results  *****\n",
      "08/19/2022 13:27:37 - INFO - __main__ -     perplexity = tensor(2.5510)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(2.5510)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eDkPEuvbD47"
   },
   "source": [
    "## Chatting with the character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjVqotI05gOS"
   },
   "source": [
    "The model is ready, so it's time to chat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Who are you?\n",
      "SeinBot: I'm him.\n",
      ">> User:What do you do for a living?\n",
      "SeinBot: Im an architect.\n",
      ">> User:What buildings have you designed?\n",
      "SeinBot: Uh, I uh, I've designed some uh... some pretty nifty buildings.\n",
      ">> User:Maybe you can show me the buildings sometime\n",
      "SeinBot: !!! remindme 30 seconds\n",
      ">> User:What's your take on society in general?\n",
      "SeinBot: ! remind me 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=1000, \n",
    "        top_p=0.7,\n",
    "        num_beams=10,\n",
    "        temperature = 0.9,\n",
    "        #num_beams=5, \n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Who are you?\n",
      "SeinBot: I'm him.\n",
      ">> User:Can you be more specific?\n",
      "SeinBot: Specific, I want specific.\n",
      ">> User:Tell me your real name\n",
      "SeinBot: Stephens.\n",
      ">> User:Tell me your first and last name\n",
      "SeinBot: Nelson.\n",
      ">> User:I thought you were george costanza\n",
      "SeinBot: !!!,!!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=10000, \n",
    "        num_beams=15,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:What do you think about Jerry Seinfeld?\n",
      "SeinBot: Let me hear your thoughts.\n",
      ">> User:I asked you first\n",
      "SeinBot: I'm Cuban.\n",
      ">> User:You make no sense\n",
      "SeinBot: I know what I'm talking about.\n",
      ">> User:Seems like you are lying\n",
      "SeinBot: !!!?!!,!!.\n",
      ">> User:ok\n",
      "SeinBot: !?,!,,!??!?...!!...!?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=10000, \n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:What do you think of Jerry Seinfeld?\n",
      "SeinBot:  I don't know.\n",
      ">> User:Do you like your parents?\n",
      "SeinBot: [] No.\n",
      ">> User:What's your take on bathrooms\n",
      "SeinBot: I think it's fantastic.\n",
      ">> User:What is your job?\n",
      "SeinBot: I'm an architect.\n",
      ">> User:What kind of buildings have you designed?\n",
      "SeinBot: !!!,!!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=500, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.9,\n",
    "        #num_beams=5, \n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIeqMwZktv7N",
    "outputId": "39bd0e66-5c29-42d5-b94a-a9a70a41aa58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Who are you\n",
      "SeinBot: I'm him.\n",
      ">> User:What do you think of Jerry Seinfeld?\n",
      "SeinBot: I like him. He's honest.\n",
      ">> User:That's great. But even when he lied about having cancer?\n",
      "SeinBot: That was a different time.\n",
      ">> User:You're a good friend. So anyway which movie do you want to see?\n",
      "SeinBot: !!! remindme 30 seconds\n",
      ">> User:That is not the answer I was hoping for. What are you doing later today?\n",
      "SeinBot: ?!!?!??!.!!.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output_diablomedium')\n",
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=500, \n",
    "        top_p=0.7,\n",
    "        temperature = 0.5,\n",
    "        num_beams=100, \n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"SeinBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNPpVtamLDKn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "1jAMH0XPuE-f"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjd1pCZxukWh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofg7iJNFyjBs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPTSein.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
